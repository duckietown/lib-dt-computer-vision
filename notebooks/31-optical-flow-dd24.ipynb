{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ground Projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "from dt_computer_vision.camera import CameraModel\n",
    "from dt_computer_vision.camera.calibration.extrinsics.boards import CalibrationBoardDD24\n",
    "\n",
    "this_dir: str = os.path.abspath(\"\")\n",
    "assets_dir: str = os.path.join(this_dir, \"..\", \"assets\")\n",
    "image_fpath: str = os.path.join(\n",
    "    assets_dir, \"extrinsics/dd24/real-world/scenario0/image-0.png\"\n",
    ")\n",
    "image: np.ndarray = cv2.imread(image_fpath)\n",
    "board = CalibrationBoardDD24\n",
    "\n",
    "# Load camera model from YAML file\n",
    "yaml_file = os.path.join(\n",
    "    assets_dir,\n",
    "    \"extrinsics\",\n",
    "    \"dd24\",\n",
    "    \"real-world\",\n",
    "    \"scenario1\",\n",
    "    \"calibration-intrinsic-dd24.yaml\",\n",
    ")\n",
    "\n",
    "with open(yaml_file, \"r\") as file:\n",
    "    yaml_content = file.read()\n",
    "    camera = CameraModel.from_ros_calibration(yaml_content)\n",
    "\n",
    "assert image.shape == (\n",
    "    camera.height,\n",
    "    camera.width,\n",
    "    3,\n",
    "), f\"Image shape: {image.shape}, Camera shape: {*camera.get_shape(), 3}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load ground homography from YAML file\n",
    "import os\n",
    "import yaml\n",
    "\n",
    "from dt_computer_vision.camera.homography import Homography\n",
    "import numpy as np\n",
    "\n",
    "this_dir: str = os.path.abspath(\"\")\n",
    "assets_dir: str = os.path.join(this_dir, \"..\", \"assets\")\n",
    "\n",
    "yaml_file = os.path.join(\n",
    "    assets_dir,\n",
    "    \"extrinsics\",\n",
    "    \"dd24\",\n",
    "    \"real-world\",\n",
    "    \"scenario0\",\n",
    "    \"homography.yaml\",\n",
    ")\n",
    "\n",
    "with open(yaml_file, \"r\") as file:\n",
    "    yaml_content = file.read()\n",
    "    homography = yaml.safe_load(yaml_content)\n",
    "\n",
    "H = Homography(np.array(homography[\"homography\"]).reshape(3, 3))\n",
    "\n",
    "print(H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from dt_computer_vision.camera.homography import interpolate_homography\n",
    "\n",
    "VIRTUAL_CAMERA_HEIGHT = 0.3\n",
    "\n",
    "image = cv2.imread(\n",
    "    os.path.join(assets_dir, \"extrinsics/dd24/real-world/scenario0/image-0.png\")\n",
    ")\n",
    "\n",
    "R2 = np.eye(3)\n",
    "tvec2 = np.array([0, 0.0, VIRTUAL_CAMERA_HEIGHT]).reshape(3, 1)\n",
    "\n",
    "H_dt = interpolate_homography(H, tvec2, R2, camera)\n",
    "\n",
    "print(\"Interpolated homography:\", H_dt)\n",
    "\n",
    "camera.H = H_dt\n",
    "camera.__post_init__()\n",
    "rectified = camera.rectifier.rectify(image)\n",
    "\n",
    "img = cv2.warpPerspective(rectified, H_dt, (camera.width, camera.height))\n",
    "plt.imshow(img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate over each pixel of the rectified image\n",
    "\n",
    "destination_img = np.zeros((camera.height, camera.width, 3), dtype=np.uint8)\n",
    "\n",
    "print(H_dt.inverse)\n",
    "\n",
    "for y in range(camera.height):\n",
    "    for x in range(camera.width):\n",
    "        # project the pixel to the ground plane\n",
    "        p = np.array([x, y, 1]).reshape(3, 1)\n",
    "        p_ground = H_dt.inverse @ p\n",
    "        p_ground /= p_ground[2]\n",
    "\n",
    "        if 0 <= p_ground[0] < camera.width and 0 <= p_ground[1] < camera.height:\n",
    "            destination_img[y, x] = rectified[int(p_ground[1]), int(p_ground[0])]\n",
    "\n",
    "plt.imshow(destination_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debugging pipeline\n",
    "\n",
    "This pipeline is used for debugging purposes, as projecting each image is computationally expensive.\n",
    "\n",
    "The pipeline for each image is:\n",
    "\n",
    "1. Rectification\n",
    "1. Projection (using the synthetized `H_dt` homography)\n",
    "1. Motion vectors computation.\n",
    "1. Debug visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Optional\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "import imageio\n",
    "from IPython.display import Image, display\n",
    "import io\n",
    "\n",
    "from dt_computer_vision.camera.types import Rectifier\n",
    "\n",
    "# Load the image\n",
    "image = cv2.imread(\n",
    "    os.path.join(assets_dir, \"extrinsics/dd24/real-world/scenario0/image-0.png\")\n",
    ")\n",
    "\n",
    "\n",
    "def simulate_translation(\n",
    "    image, H_dt : Optional[Homography], camera : CameraModel, rectifier: Optional[Rectifier], translation_time : float = 3.0, num_frames : int = 30\n",
    ") -> List:\n",
    "    # Get the image width and height\n",
    "    height, width = image.shape[:2]\n",
    "\n",
    "    # Define the translation amount\n",
    "    translation_amount = int(width / 2)\n",
    "\n",
    "    # Create a black image with the same size as the original image\n",
    "    black_image = np.zeros_like(image)\n",
    "\n",
    "    # Generate the sequence of translated images\n",
    "    sequence = []\n",
    "    if rectifier is not None:\n",
    "        # Apply the rectification to the image\n",
    "        rectified_image = rectifier.rectify(image)\n",
    "\n",
    "    for type in range(num_frames):\n",
    "        # Calculate the translation amount for the current frame\n",
    "        translation = int((type / num_frames) * translation_amount)\n",
    "\n",
    "        # Create a translation matrix\n",
    "        M = np.array([[1, 0, translation], [0, 1, 0]], dtype=np.float32)\n",
    "\n",
    "        if H_dt is not None:\n",
    "            # Apply the homography to the image\n",
    "            image = cv2.warpPerspective(rectified_image, H_dt, (camera.width, camera.height))\n",
    "\n",
    "        # Apply the translation to the original image\n",
    "        translated_image = cv2.warpAffine(\n",
    "            image,\n",
    "            M,\n",
    "            (width, height),\n",
    "            borderMode=cv2.BORDER_CONSTANT,\n",
    "            borderValue=(0, 0, 0),\n",
    "        )\n",
    "\n",
    "        # Combine the translated image with the black image to fill the missing pixels\n",
    "        final_image = cv2.bitwise_or(translated_image, black_image)\n",
    "\n",
    "        # Add the final image to the sequence\n",
    "        sequence.append(final_image)\n",
    "\n",
    "    return sequence\n",
    "\n",
    "\n",
    "translation_time = 3\n",
    "\n",
    "sequence_top_view = simulate_translation(\n",
    "    image, H_dt, camera, camera.rectifier, translation_time, num_frames=30\n",
    ")\n",
    "\n",
    "\n",
    "def gif_from_images(images: List, duration: float) -> Image:\n",
    "    buffer = io.BytesIO()\n",
    "    imageio.mimsave(buffer, images, format=\"GIF\", duration=duration)\n",
    "    buffer.seek(0)\n",
    "    return Image(data=buffer.read(), format=\"GIF\")\n",
    "\n",
    "\n",
    "# Display the gif\n",
    "display(gif_from_images(sequence_top_view, translation_time / len(sequence_top_view)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def gif_from_images(images: List, duration: float) -> Image:\n",
    "    buffer = io.BytesIO()\n",
    "    imageio.mimsave(buffer, images, format=\"GIF\", duration=duration)\n",
    "    buffer.seek(0)\n",
    "    return Image(data=buffer.read(), format=\"GIF\")\n",
    "\n",
    "\n",
    "# Display the gif\n",
    "display(gif_from_images(sequence_top_view, translation_time / len(sequence_top_view)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "from IPython.display import display\n",
    "\n",
    "# Compute optical flow using the OpticalFlow class\n",
    "from dt_computer_vision.optical_flow.optical_flow import OpticalFlow\n",
    "\n",
    "\n",
    "config = {\n",
    "    \"process_frequency\": 20,\n",
    "    \"track_len\": 10,\n",
    "    \"detect_interval\": 5,\n",
    "    \"img_scale\": 1,\n",
    "}\n",
    "\n",
    "VIS_SCALE = int(1 / config[\"img_scale\"])\n",
    "\n",
    "# Create an instance of the OpticalFlow class\n",
    "optical_flow = OpticalFlow(\n",
    "    track_len=config[\"track_len\"],\n",
    "    detect_interval=config[\"detect_interval\"],\n",
    "    resize_scale=config[\"img_scale\"],\n",
    ")\n",
    "\n",
    "SEQUENCE_FRAME_COUNT = len(sequence_top_view)\n",
    "delta_t = translation_time / SEQUENCE_FRAME_COUNT\n",
    "\n",
    "debug_images = []\n",
    "debug_strs = []\n",
    "disp_vectors = []\n",
    "vel_motion_vectors = []\n",
    "features_locations = []\n",
    "\n",
    "# Iterate over sequence_top_view picking two consecutive frames\n",
    "for image in sequence_top_view:\n",
    "    # Compute the optical flow\n",
    "    displacements_array, velocities_arr, locations_px, debug_str = (\n",
    "        optical_flow.compute_motion_vectors(image, delta_t)\n",
    "    )\n",
    "\n",
    "    vis = optical_flow.create_debug_visualization(\n",
    "        image, locations_px, debug_str\n",
    "    )\n",
    "\n",
    "    debug_images.append(vis)\n",
    "    features_locations.append(locations_px)\n",
    "    disp_vectors.append(displacements_array)\n",
    "    vel_motion_vectors.append(velocities_arr)\n",
    "    debug_strs.append(debug_str)\n",
    "print(f\"The debug image has shape: {debug_images[0].shape}\")\n",
    "\n",
    "# Display the optical flow visualization\n",
    "display(gif_from_images(debug_images, delta_t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Production pipeline\n",
    "\n",
    "1. Motion vectors computation\n",
    "1. Rectification of motion vectors\n",
    "1. Projection of motion vectors\n",
    "1. Debug visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Generate a sequence of unprojected images equivalent to the sequence of projected images.\n",
    "\"\"\"\n",
    "# Load the image\n",
    "image = cv2.imread(\n",
    "    os.path.join(assets_dir, \"extrinsics/dd24/real-world/scenario0/image-0.png\")\n",
    ")\n",
    "\n",
    "sequence = simulate_translation(image, H_dt, camera, camera.rectifier, translation_time)\n",
    "\n",
    "sequence = [\n",
    "    camera.rectifier.distort(\n",
    "        cv2.warpPerspective(image, np.linalg.inv(H_dt), (camera.height, camera.width))\n",
    "    )\n",
    "    for image in sequence\n",
    "]\n",
    "\n",
    "display(gif_from_images(sequence, delta_t))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dt_computer_vision.camera.types import Pixel\n",
    "from dt_computer_vision.optical_flow.optical_flow import OpticalFlow\n",
    "\n",
    "optical_flow = OpticalFlow(\n",
    "    track_len=config[\"track_len\"],\n",
    "    detect_interval=config[\"detect_interval\"],\n",
    "    resize_scale=config[\"img_scale\"],\n",
    ")\n",
    "\n",
    "VIS_SCALE = 1\n",
    "\n",
    "\n",
    "SEQUENCE_FRAME_COUNT = len(sequence)\n",
    "delta_t = translation_time / SEQUENCE_FRAME_COUNT\n",
    "\n",
    "debug_images = []\n",
    "debug_strs = []\n",
    "disp_vectors = []\n",
    "vel_motion_vectors = []\n",
    "features_locations = []\n",
    "debug_images_projected = []\n",
    "\n",
    "# Iterate over sequence picking two consecutive frames\n",
    "for image in sequence:\n",
    "    # Rectify the image\n",
    "    image = camera.rectifier.rectify(image)\n",
    "\n",
    "    # Compute the optical flow\n",
    "    displacements_array, velocities_arr, locations_px, debug_str = (\n",
    "        optical_flow.compute_motion_vectors(image, delta_t)\n",
    "    )\n",
    "\n",
    "    projected_motion_vectors, projected_locations = optical_flow.project_motion_vectors(\n",
    "        velocities_arr, locations_px, camera, H_dt\n",
    "    )\n",
    "\n",
    "    locations_px_resized = [\n",
    "        Pixel(*(loc / optical_flow.resize_scale).as_array()) for loc in locations_px\n",
    "    ]\n",
    "\n",
    "    projected_image = cv2.warpPerspective(image, H_dt, (camera.width, camera.height))\n",
    "\n",
    "    vis_original = optical_flow.create_debug_visualization(\n",
    "        image, locations_px_resized, debug_str, motion_vectors=velocities_arr\n",
    "    )\n",
    "\n",
    "    vis = optical_flow.create_debug_visualization(\n",
    "        projected_image,\n",
    "        projected_locations,\n",
    "        debug_str,\n",
    "        motion_vectors=velocities_arr,\n",
    "    )\n",
    "\n",
    "    debug_images.append(vis_original)\n",
    "    debug_images_projected.append(vis)\n",
    "    features_locations.append(locations_px_resized)\n",
    "    disp_vectors.append(displacements_array)\n",
    "    vel_motion_vectors.append(velocities_arr)\n",
    "    debug_strs.append(debug_str)\n",
    "\n",
    "# Display the optical flow visualizations side by side\n",
    "display(gif_from_images(debug_images, delta_t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the projected motion vectors\n",
    "display(gif_from_images(debug_images_projected, delta_t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Measure the performance of the optical flow algorithm\n",
    "\n",
    "import time\n",
    "\n",
    "t_start = time.perf_counter()\n",
    "\n",
    "base_homography_pixel_per_meter = 900\n",
    "\n",
    "velocities_px_s = []\n",
    "velocities_m_s = []\n",
    "\n",
    "for image in sequence:\n",
    "    # Rectify the image\n",
    "    image = camera.rectifier.rectify(image)\n",
    "\n",
    "    # Compute the optical flow\n",
    "    displacements_array, velocities_arr, locations_px, debug_str = (\n",
    "        optical_flow.compute_motion_vectors(image, delta_t)\n",
    "    )\n",
    "\n",
    "    projected_motion_vectors, projected_locations = optical_flow.project_motion_vectors(\n",
    "        velocities_arr, locations_px, camera, H_dt\n",
    "    )\n",
    "    velocity = optical_flow.compute_velocity_vector(velocities_arr)\n",
    "    velocities_px_s.append(velocity)\n",
    "    velocities_m_s.append(velocity / base_homography_pixel_per_meter)\n",
    "    \n",
    "t_end = time.perf_counter()\n",
    "\n",
    "print(f\"Optical flow ran at {len(sequence) / (t_end - t_start)} FPS\")\n",
    "print(f\"Mean velocity in pixels per second: {np.mean(velocities_px_s)}\")\n",
    "print(f\"Mean velocity in meters per second: {np.mean(velocities_m_s)}\")"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
